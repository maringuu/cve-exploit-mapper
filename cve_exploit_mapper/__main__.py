import argparse
from argparse import ArgumentParser

import scraper
import sys
import scraper.metasploit
import scraper.exploit_db
import scraper.trickest
import pandas as pd
from pandas import DataFrame
import os
import logging



parser = ArgumentParser()
parser.add_argument(
    "--outfile",
    default="cve_exploit_mapping.csv",
    help="Path to the output csv file",
)
parser.add_argument(
    "--infile",
    help="Path to an input file that shall be updated. Can be the same as --outfile",
)
parser.add_argument(
    "--scrapers",
    default="trickest,metasploit,exploitdb",
    help="""Comma separated list of scrapers to be used
    Avaiable scrapers: trickest, metasploit, exploitdb
    """
)
parser.add_argument(
    "-v",
    "--verbose",
    action="store_true",
)

args = parser.parse_args()

scrapers = args.scrapers.split(",")
assert set(scrapers).issubset({"metasploit", "trickest", "exploitdb"})

outfile = open(args.outfile, "w+") if args.outfile != "-" else sys.stdout
infile = args.infile

loglevel = logging.DEBUG if args.verbose else logging.INFO


logging.basicConfig(
    # TODO better formatting
    level=loglevel,
)
# Don't clutter the logging with urllib logs
requests_log = logging.getLogger("urllib3")
requests_log.setLevel(logging.INFO)

df = DataFrame(columns=["cve", "exploit-link", "source"])
if infile is not None:
    infile_df = pd.read_csv(infile, names=["cve", "exploit-link", "source"])
    df = pd.concat([df, infile_df])
    logging.debug(f"Loaded infile from {infile} with {df.shape[0]} rows.")

cache_path = os.getenv("XDG_CACHE_HOME", f"{os.getenv('HOME')}/.cache") + "/cve_exploit_mapper/"
logging.debug(f"Cache diretory: {cache_path}")

metasploit_df = None
if "metasploit" in scrapers:
    metasploit_path = cache_path + "/metasploit"
    scraper.metasploit.download_to_path(metasploit_path)
    metasploit_df = scraper.metasploit.dataframe_from_directory(metasploit_path)
    logging.info(f"Got {metasploit_df.shape[0]} CVEs/exploits from metasploit")

trickest_df = None
if "trickest" in scrapers:
    trickest_path = cache_path + "/trickest"
    scraper.trickest.download_to_path(trickest_path)
    trickest_df = scraper.trickest.dataframe_from_directory(trickest_path)
    logging.info(f"Got {trickest_df.shape[0]} CVEs/exploits from trickest")

exploitdb_df = None
if "exploitdb" in scrapers:
    skip_path = cache_path + "/exploitdb-skip.csv"
    try:
        skip_df = pd.read_csv(skip_path)
        logging.debug(f"Loaded ExploitDB skipfile from {skip_path} with {skip_df.shape[0]} rows.")
    except FileNotFoundError:
        # We dont care if the file does not exist
        logging.debug("No ExploitDB skipfile found")
        skip_df = None

    exploitdb_path = cache_path + "/exploitdb"
    scraper.exploit_db.download_to_path(exploitdb_path)
    exploitdb_df, no_cve_df = scraper.exploit_db.fetch_dataframe(exploitdb_path, delta=df, skip=skip_df)
    logging.info(f"Got {exploitdb_df.shape[0]} CVEs/exploits from ExploitDB")

    skip_df = pd.concat([skip_df, no_cve_df])
    skip_df.to_csv(skip_path, index=False)

df = pd.concat([df, metasploit_df, trickest_df, exploitdb_df])
# As we only do a delta update on exploitdb the others will cause duplicates
df.drop_duplicates(inplace=True)
logging.info(f"Got {df.shape[0]} CVEs/exploits from all sources")
df.to_csv(outfile, index=False)
